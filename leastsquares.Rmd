---
title: "Least Squares of Multiple Discretization Results for One Data Set"
author: "Tiffany Jann"
date: ""
output: 
  html_document:
    fig_height: 3
    fig_width: 5
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE, echo=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments("leastsquares.Rmd")
```
<!-- Don't edit the material above this line -->
```{r include=FALSE}
options(scipen=999)
```

Variable inputs:
```{r}
#Gold Standard Network Data
#Change header value to TRUE or FALSE (MULAN data will always be FALSE)
goldFile = "forty_ninefmriCS100S20N204.csv"
```

Helper: reads and transposes data table
```{r}
#Note: header=FALSE if in silico data, else TRUE
read.tcsv = function(file, header=TRUE, sep=",", ...) {

  n = max(count.fields(file, sep=sep), na.rm=TRUE)
  x = readLines(file)

  .splitvar = function(x, sep, n) {
    var = unlist(strsplit(x, split=sep))
    length(var) = n
    return(var)
  }

  x = do.call(cbind, lapply(x, .splitvar, sep=sep, n=n))
  x = apply(x, 1, paste, collapse=sep) 
  out = read.csv(text=x, sep=sep, header=header)
  return(out)

}
```

Load and normalize original gold standard network
```{r}
gold = read.tcsv(goldFile, header=FALSE)

#A difference vector gold for TSD and Erdal's (change between time points)
intvlGold = apply(gold, 2, diff)

dim(gold)
summary(gold)
print(min(gold))
#Vertical shift
gold = gold - min(gold)
#Should be 0
print(min(gold))
#Scaling to unit length
gold = gold / (max(gold) - min(gold))
#Normalized gold standard network
cat("min:", min(gold), "max:", max(gold))

dim(intvlGold)
summary(intvlGold)
print(min(intvlGold))
#Vertical shift
intvlGold = intvlGold - min(intvlGold)
#Should be 0
print(min(intvlGold))
#Scaling to unit length
intvlGold = intvlGold / (max(intvlGold) - min(intvlGold))
#Normalized gold standard network
cat("min:", min(intvlGold), "max:", max(intvlGold))
```

Find all applicable files w.r.t. gold standard network
```{r}
#make sure data file follows [identifying name]fmri[other unimportant identifiers].csv
key = gsub("fmri.*\\.csv$", "", goldFile)
pattern = paste(key, "_", sep="")

#gets files that start with [identifying name]_
allFiles = list.files(getwd(), pattern)
allFiles

#Separate the file names for those looking at time points from those looking at difference between times
pointFiles = allFiles[!grepl("tsd", allFiles) & !grepl("erdals", allFiles)]
intervalFiles = allFiles[grepl("tsd", allFiles) | grepl("erdals", allFiles)]
pointFiles
intervalFiles
```

Helper: gets all discretization results ready for evaluation
```{r}
getfile = function(file){
  discretized = read.tcsv(file)
  rownames = discretized[,1]
  discretized = na.omit(discretized[,-1])
  #Ensures data uses 0 1 instead of 1 2
  if(max(discretized) > 1){
    discretized = discretized - max(discretized) + 1
  }
  return(discretized)
}
```

Load all files using helper `getfile`
```{r}
pointData = sapply(pointFiles, getfile)
intervalData = sapply(intervalFiles, getfile)
head(pointData)
head(intervalData)
# allData = sapply(allFiles, getfile)
```

Helper: calculate aggregate error under all nodes of **one** discretization output
```{r}
score = function(discretized, standard){
  methodError = 0
  for(node in 1:length(discretized)){
    methodError = methodError + sum( abs(discretized[node] - standard[node]))
  }
  return(methodError)
}
```

Apply all `disc` files to helper `score`
```{r, warning=FALSE}
scores = append(apply(pointData, 2, score, gold), apply(intervalData, 2, score, as.data.frame(intvlGold)))
```

View results, best method first
```{r}
sort(scores)
```